\relax 
\providecommand\hyper@newdestlabel[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{optimization}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Optimization methods}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}First-order optimization methods}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Gradient Descent}{1}{subsubsection.1.1.1}\protected@file@percent }
\citation{optimization}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An example of choosing a large and small learning rate value.\relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:learningrate}{{1.1}{2}{An example of choosing a large and small learning rate value.\relax }{figure.caption.1}{}}
\citation{videocls}
\citation{optimization}
\citation{SGD}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Linear Regression model is basically represented on graph\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:linearregression}{{1.2}{3}{Linear Regression model is basically represented on graph\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Stochastic gradient descent}{3}{subsubsection.1.1.2}\protected@file@percent }
\citation{optimization}
\citation{GD}
\citation{optimization}
\citation{LSDDeepNet}
\citation{optimization}
\citation{optimization}
\citation{GD}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Adaptive learning rate}{4}{subsubsection.1.1.3}\protected@file@percent }
\citation{GD}
\citation{GD}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Summary table}{5}{subsubsection.1.1.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of Optimization Algorithms\relax }}{5}{table.caption.3}\protected@file@percent }
\citation{optimization}
\citation{newton}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}High-order optimization}{6}{subsection.1.2}\protected@file@percent }
\newlabel{hessian}{{11}{6}{High-order optimization}{equation.1.11}{}}
\citation{newton}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces The tangent lines of $x^3 + 2x + 2$ at 0 and 1 intersect the x-axis at $(1, 0)$ and $(0, 2)$ respectively and the solution is a point close to -2. In this case, Newton's method never converges.\relax }}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:newtonmethodtangentline}{{1.3}{7}{The tangent lines of $x^3 + 2x + 2$ at 0 and 1 intersect the x-axis at $(1, 0)$ and $(0, 2)$ respectively and the solution is a point close to -2. In this case, Newton's method never converges.\relax }{figure.caption.4}{}}
\citation{optimization}
\citation{newton}
\citation{oBFGS}
\citation{SQNM}
\citation{SQNM}
\citation{SQNAlgo}
\citation{optimization}
\citation{optimization}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Stochastic Quasi-Newton Method \cite  {SQNAlgo}\relax }}{8}{algocf.1}\protected@file@percent }
\newlabel{algo:SQNAlgorithm}{{1}{8}{High-order optimization}{algocf.1}{}}
\citation{optimization}
\citation{derivative-free}
\citation{contlearning}
\citation{contlearning}
\citation{catastrophic}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Derivative-free optimization}{9}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Continual learning and Test production in building a ML solution}{9}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Continual learning}{9}{subsection.2.1}\protected@file@percent }
\citation{contlearning}
\citation{contlearning}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparation and Evaluation of Optimization Methods\relax }}{10}{table.caption.6}\protected@file@percent }
\newlabel{Tab:evaluation}{{2}{10}{Comparation and Evaluation of Optimization Methods\relax }{table.caption.6}{}}
\newlabel{fig:semanticsegmentation}{{\caption@xref {fig:semanticsegmentation}{ on input line 492}}{10}{Continual learning}{figure.caption.7}{}}
\newlabel{fig:textsum}{{\caption@xref {fig:textsum}{ on input line 497}}{10}{Continual learning}{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Some applications of Continual Learning: Semantic Segmentation (left) and Text Summarization (right)\relax }}{10}{figure.caption.7}\protected@file@percent }
\newlabel{fig:CLapps}{{2.1}{10}{Some applications of Continual Learning: Semantic Segmentation (left) and Text Summarization (right)\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A state-of-the-art and elaborated taxonomy of representative continual learning methods. We have summarized 5 main categories (blue blocks), each of which is further divided into several sub-directions (red blocks). \cite  {contlearning}\relax }}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:methods}{{2.2}{11}{A state-of-the-art and elaborated taxonomy of representative continual learning methods. We have summarized 5 main categories (blue blocks), each of which is further divided into several sub-directions (red blocks). \cite {contlearning}\relax }{figure.caption.9}{}}
\citation{contlearning}
\citation{cont-regular}
\citation{contlearning}
\citation{contlearning}
\citation{contlearning}
\citation{test}
\citation{test}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Test production}{14}{subsection.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Machine Learning testing flow \cite  {test}\relax }}{14}{figure.caption.12}\protected@file@percent }
\newlabel{fig:test-flow}{{2.3}{14}{Machine Learning testing flow \cite {test}\relax }{figure.caption.12}{}}
\citation{test}
\citation{test}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Compare online learning and batch learning with respect to the criteria of latency testing and load testing in the post-training test stage. \cite  {test}\relax }}{16}{figure.caption.13}\protected@file@percent }
\newlabel{fig:post-test}{{2.4}{16}{Compare online learning and batch learning with respect to the criteria of latency testing and load testing in the post-training test stage. \cite {test}\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A/B Testing steps demonstration\relax }}{16}{figure.caption.14}\protected@file@percent }
\newlabel{fig:abtest}{{2.5}{16}{A/B Testing steps demonstration\relax }{figure.caption.14}{}}
\bibcite{optimization}{1}
\bibcite{GD}{2}
\bibcite{SGD}{3}
\bibcite{videocls}{4}
\bibcite{LSDDeepNet}{5}
\bibcite{newton}{6}
\bibcite{oBFGS}{7}
\bibcite{SQNM}{8}
\bibcite{SQNAlgo}{9}
\bibcite{derivative-free}{10}
\bibcite{contlearning}{11}
\bibcite{catastrophic}{12}
\bibcite{cont-regular}{13}
\bibcite{test}{14}
